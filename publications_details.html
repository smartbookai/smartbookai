
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>MCER &mdash; Maqam Center for Educational Research</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Hydro-climatological Lab" />
	<meta name="keywords" content="Muhammed Swavaf, MCER" />
	<meta name="author" content="Muhammed Swavaf, MCER" />

  	<!-- 
	//////////////////////////////////////////////////////

	FREE HTML5 TEMPLATE 
	DESIGNED & DEVELOPED by FreeHTML5.co
		
	Website: 		http://freehtml5.co/
	Email: 			info@freehtml5.co
	Twitter: 		http://twitter.com/fh5co
	Facebook: 		https://www.facebook.com/fh5co

	//////////////////////////////////////////////////////
	-->

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="fh5co-page">
		<a href="#" class="js-fh5co-nav-toggle fh5co-nav-toggle"><i></i></a>
		<aside id="fh5co-aside" role="complementary" class="border js-fullheight">

			<h1 id="fh5co-logo"><a href="index.html">MCER</a></h1>
			<nav id="fh5co-main-menu" role="navigation">
				<ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="about.html">About</a></li>
                    <li><a href="research.html">Research</a></li>
                    <li><a href="accomplishment.html">Accomplishments</a></li>
                    <li class="fh5co-active"><a href="publications_details.html">Publication Details</a></li>
                    <!-- <li><a href="teaching.html">Teaching
                    </a></li>
                    <li><a href="people.html">People</a></li>
                    <li><a href="contact.html">Join us</a></li>
                    <li><a href="gallery.html">Gallery</a></li> -->
				</ul>
			</nav>

			<div class="fh5co-footer">
                <p><small>&copy; 2024 Muhammed Swavaf. </small></p>

				<ul>
					<!--<li><a href="#"><i class="icon-facebook2"></i></a></li>
					<li><a href="#"><i class="icon-twitter2"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>-->
					<li><a href="https://www.linkedin.com/in/muhammed-swavaf/"><i class="icon-linkedin2"></i></a></li>
				</ul>
			</div>

		</aside>
        <div id="fh5co-main">

                <h2 class="pubHeading">JOURNAL PUBLICATIONS</h2>
                <div class="publicationContainer">

                                <div class="publicationInner">

                                        <div class="pubHead">

                                            <h3>Evaluating segment anything model (SAM) on MRI scans of brain tumors</h3>
                                        </div>

                                        <div class="pubAuthors">

                                           <p>Luqman Ali, Fady Alnajjar, Muhammed Swavaf, Omar Elharrouss, Alaa Ali Abd-alrazaq, Rafat Damseh, </p>
                                        </div>

                                        <div class="pubClass">

                                            <p>Scientific Reports 14(1)</p>
                                        </div>

                                        <div class="pubAbstract">

                                            <span>ABSTRACT: </span><p align="justify">Addressing the challenge of automatically segmenting anatomical structures from brain images has been a long-standing problem, attributed to subject- and image-based variations and constraints in available data annotations. The Segment Anything Model (SAM), developed by Meta, is a foundational model trained to provide zero-shot segmentation outputs with or without interactive user inputs, demonstrating notable performance on various objects and image domains without explicit prior training. This study evaluated SAM’s performance in brain tumor segmentation using two publicly available Magnetic Resonance Imaging (MRI) datasets. The study analyzed SAM’s standalone segmentation as well as its performance when provided user interaction through point prompts and bounding box inputs. SAM exhibited versatility across configurations and datasets, with the bounding box consistently outperforming others in achieving superior localized precision, with average Dice scores of 0.68 for TCGA and 0.56 for BRATS, along with average IoU values of 0.89 and 0.65, respectively, especially for tumors with low-to-medium curvature. Inconsistencies were observed, particularly in relation to variations in tumor size, shape, and textural features. The conclusion drawn from the study is that while SAM can automate medical image segmentation, further training and careful implementation are necessary for diagnostic purposes, especially with challenging cases such as MRI scans of brain tumors.</p>

                                        </div>

                                        
                                            <div class="pubPageLink">

                                                <a class="btn" href="https://www.nature.com/articles/s41598-024-72342-x">DOI</a>

                                            </div>
											<div class="col-md-9 animate-box" data-animate-effect="fadeInLeft">
												<center><img class="img-responsive" src="images/z11154wn.png" /> <p align="justify"></p> </center>
											</div>
                                           

                                </div>

                            </div>
                <!-- Publication -->
                            <div class="publicationContainer">

                                <div class="publicationInner">

                                        <div class="pubHead">

                                            <h3>Rs-net: Residual Sharp U-Net architecture for pavement crack segmentation and severity assessment</h3>
                                        </div>

                                        <div class="pubAuthors">

                                            <p>Luqman Ali, Hamad Aljassmi, Muhammed Swavaf, Wasif Khan, Fady Alnajjar, </p>

                                        </div>

                                        <div class="pubClass">

                                            <p>Journal of Big Data</p>

                                        </div>

                                        <div class="pubAbstract">

                                            <span>ABSTRACT: </span><p align="justify">U-net, a fully convolutional network-based image segmentation method, has demonstrated widespread adaptability in the crack segmentation task. The combination of the semantically dissimilar features of the encoder (shallow layers) and the decoder (deep layers) in the skip connections leads to blurry features map and leads to undesirable over- or under-segmentation of target regions. Additionally, the shallow architecture of the U-Net model prevents the extraction of more discriminatory information from input images. This paper proposes a Residual Sharp U-Net (RS-Net) architecture for crack segmentation and severity assessment in pavement surfaces to address these limitations. The proposed architecture uses residual block in the U-Net model to extract a more insightful representation of features. In addition to that, a sharpening kernel filter is used instead of plain skip connections to generate a fine-tuned encoder features map before combining it with decoder features maps to reduce the dissimilarity between them and smoothes artifacts in the network layers during early training. The proposed architecture is also integrated with various morphological operations to assess the severity of cracks and categorize them into hairline, medium, and severe labels. Experiments results demonstrated that the RS-Net model has promising segmentation performance, outperforming earlier U-Net variations on testing data for crack segmentation and severity assessment, with a promising accuracy (>0.97)</p>

                                        </div>

                                        
                                            <div class="pubPageLink">

                                                <a class="btn" href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00981-y">DOI</a>

                                            </div>
											<div class="col-md-9 animate-box" data-animate-effect="fadeInLeft">
												<center><img class="img-responsive" src="images/zx64shmp.png" /> <p align="justify"></p> </center>
											</div>
                                            

                                </div>

                            </div>
                <!-- Publication -->
                            <div class="publicationContainer">

                                <div class="publicationInner">

                                        <div class="pubHead">

                                            <h3>Navigating the YOLO Landscape: A Comparative Study of Object Detection Models for Emotion Recognition</h3>

                                        </div>

                                        <div class="pubAuthors">

                                            <p>Medha Mohan Ambali Parambil, Luqman Ali, Muhammed Swavaf, Salah Bouktif, Munkhjargal Gochoo, Hamad Aljassmi, Fady Alnajjar, </p>

                                        </div>

                                        <div class="pubClass">

                                            <p>IEEE</p>

                                        </div>

                                        <div class="pubAbstract">

                                            <span>ABSTRACT: </span><p align="justify">The You Only Look Once (YOLO) series, renowned for its efficiency and versatility in object detection, has become a fundamental component in diverse fields ranging from autonomous vehicles to robotics and video surveillance. Despite its widespread application, a notable gap exists in the literature concerning selecting YOLO models for specific tasks. Current trends often lean towards the latest models, potentially overlooking crucial factors such as computational complexity, speed, accuracy, model size, adaptability, and generalization. This approach may not always yield the optimal choice for a given application. Therefore, this paper aims to provide an exhaustive comparative analysis of various YOLO models, focusing on emotion recognition. We trained and tested YOLOv5, YOLOv7, YOLOv8, and YOLOv9 along with their respective variants, using a subset of AffectNet dataset, which consists of facial images annotated with one of five emotions, namely angry, happy, sad, neutral, and surprise. The study evaluates the models based on several key parameters: accuracy using metrics like mean Average Precision (mAP), inference time, FPS, model size, adaptability to altered datasets, and generalization capability. Comprehensive results are presented, highlighting the strengths and limitations of each model variant across these parameters. Insights are provided to guide researchers in selecting the most suitable YOLO architecture for their specific emotion recognition requirements, considering factors such as computational constraints, real-time performance needs, and the importance of accuracy vs efficiency tradeoffs. The analysis reveals the exceptional performances of certain models like YOLOv9e for high accuracy and YOLOv8n for balancing speed and accuracy. Overall, this work fills a crucial gap by offering a detailed comparative study to facilitate informed decision-making when deploying YOLO for facial emotion recognition tasks.</p>

                                        </div>

                                        
                                            <div class="pubPageLink">

                                                <a class="btn" href="https://ieeexplore.ieee.org/document/10623625">DOI</a>

                                            </div>
											<div class="col-md-9 animate-box" data-animate-effect="fadeInLeft">
												<center><img class="img-responsive" src="images/GHT.jpg" /> <p align="justify"></p> </center>
											</div>

                                </div>

                            </div>



    </div>
	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>

